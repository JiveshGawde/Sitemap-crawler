{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sitemap Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.tacchini.it/en/post-sitemap.xml', 'https://www.tacchini.it/en/page-sitemap.xml', 'https://www.tacchini.it/en/realizzazioni-sitemap.xml', 'https://www.tacchini.it/en/video-sitemap.xml', 'https://www.tacchini.it/en/designers-sitemap.xml', 'https://www.tacchini.it/en/journal-sitemap.xml', 'https://www.tacchini.it/en/tipologia-sitemap.xml', 'https://www.tacchini.it/en/tacchiniedizioni-sitemap.xml', 'https://www.tacchini.it/en/savoir-faire-sitemap.xml', 'https://www.tacchini.it/en/category-sitemap.xml', 'https://www.tacchini.it/en/realiztax-sitemap.xml', 'https://www.tacchini.it/en/videotax-sitemap.xml', 'https://www.tacchini.it/en/journaltax-sitemap.xml', 'https://www.tacchini.it/en/distributoritax-sitemap.xml', 'https://www.tacchini.it/en/tacchiniedizionitax-sitemap.xml', 'https://www.tacchini.it/en/savoir-faire-sitemap.xml']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_sitemap_urls(base_url):\n",
    "    \"\"\"Fetches URLs from a website's sitemap.xml if available.\"\"\"\n",
    "    sitemap_url = base_url.rstrip(\"/\") + \"/sitemap.xml\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        if response.status_code == 200:\n",
    "            root = ET.fromstring(response.content)\n",
    "            urls = [elem.text for elem in root.iter() if elem.tag.endswith(\"loc\")]\n",
    "            return urls\n",
    "        else:\n",
    "            print(\"No sitemap found.\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching sitemap: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "sitemap_links = get_sitemap_urls(\"https://www.tacchini.it/en/\")\n",
    "print(sitemap_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 207 URLs and saved to product_urls.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "# List of important sitemaps\n",
    "sitemaps = [\n",
    "    \"https://www.tacchini.it/en/post-sitemap.xml\"\n",
    "]\n",
    "\n",
    "def extract_urls_from_sitemap(sitemap_url):\n",
    "    \"\"\"Fetch and extract URLs from a given sitemap XML.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        if response.status_code == 200:\n",
    "            root = ET.fromstring(response.content)\n",
    "            urls = [elem.text for elem in root.iter() if elem.tag.endswith(\"loc\")]\n",
    "            return urls\n",
    "        else:\n",
    "            print(f\"Failed to fetch: {sitemap_url}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {sitemap_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Store all product-related URLs\n",
    "all_product_urls = []\n",
    "\n",
    "for sitemap in sitemaps:\n",
    "    urls = extract_urls_from_sitemap(sitemap)\n",
    "    all_product_urls.extend(urls)\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"jsons/product_urls.json\", \"w\") as f:\n",
    "    json.dump(all_product_urls, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(all_product_urls)} URLs and saved to product_urls.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamically filtered links saved to filtered_dynamic_weblinks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON file with links\n",
    "with open(\"jsons/product_urls.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    weblinks = json.load(file)\n",
    "\n",
    "# Define dynamic heuristics\n",
    "IMAGE_EXTENSIONS = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\")\n",
    "IGNORED_TERMS = [\"about\", \"contact\", \"privacy\", \"terms\", \"blog\", \"journal\", \"distributor\", \"video\"]\n",
    "\n",
    "def score_link(link):\n",
    "    \"\"\"Assigns a score to a link based on its relevance.\"\"\"\n",
    "    score = 0\n",
    "\n",
    "    # Check for images (Exclude)\n",
    "    if link.endswith(IMAGE_EXTENSIONS):\n",
    "        return -1  \n",
    "\n",
    "    # Remove homepage & short URLs\n",
    "    if link.count(\"/\") <= 4:  # E.g., 'https://www.tacchini.it/en/' has only 4 slashes\n",
    "        return -1  \n",
    "\n",
    "    # Penalize ignored terms\n",
    "    if any(term in link for term in IGNORED_TERMS):\n",
    "        return -1  \n",
    "\n",
    "    # Boost score for deep URLs (longer paths = likely useful)\n",
    "    path_parts = link.strip(\"/\").split(\"/\")\n",
    "    score += len(path_parts) * 2  \n",
    "\n",
    "    # Extra boost for meaningful words\n",
    "    for part in path_parts:\n",
    "        if len(part) > 3:  \n",
    "            score += 2\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Apply scoring to all links\n",
    "scored_links = {link: score_link(link) for link in weblinks}\n",
    "\n",
    "# Filter out irrelevant links (score = -1) and sort by score\n",
    "filtered_links = sorted(\n",
    "    [link for link, score in scored_links.items() if score > 0],\n",
    "    key=lambda x: scored_links[x],\n",
    "    reverse=True  # Higher score first\n",
    ")\n",
    "\n",
    "# Save the dynamically filtered links\n",
    "with open(\"jsons/filtered_urls.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(filtered_links, outfile, indent=4)\n",
    "\n",
    "print(\"‚úÖ Dynamically filtered links saved to filtered_urls.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping: https://www.tacchini.it/en/low-tables/daze/\n",
      "‚úÖ Scraped successfully: https://www.tacchini.it/en/low-tables/daze/\n",
      "‚úÖ Scraped HTML saved to scraped_page.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Path to your local ChromeDriver\n",
    "CHROMEDRIVER_PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "\n",
    "# Function to initialize Selenium WebDriver\n",
    "def get_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in background\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "\n",
    "    service = Service(CHROMEDRIVER_PATH)  # Use local ChromeDriver\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to extract dynamic content\n",
    "def scrape_page(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üîç Scraping: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "\n",
    "        # Expand all collapsible sections dynamically\n",
    "        expandable_elements = driver.find_elements(By.XPATH, \"//button | //summary | //div[@role='button']\")\n",
    "        for element in expandable_elements:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", element)\n",
    "                time.sleep(1)  # Wait for content to load\n",
    "            except:\n",
    "                continue  # Ignore if it can't be clicked\n",
    "\n",
    "        # Extract full HTML after interactions\n",
    "        full_html = driver.page_source\n",
    "        print(f\"‚úÖ Scraped successfully: {url}\")\n",
    "\n",
    "        return full_html\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Load links from JSON\n",
    "with open(\"jsons/filtered_urls.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    filtered_links = json.load(file)\n",
    "\n",
    "# Test one link\n",
    "test_link = filtered_links[0]\n",
    "html_content = scrape_page(test_link)\n",
    "\n",
    "# Save the extracted HTML\n",
    "if html_content:\n",
    "    with open(\"html/scraped_page.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "print(\"‚úÖ Scraped HTML saved to scraped_page.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned HTML saved to cleaned_page.html\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the scraped HTML\n",
    "with open(\"html/scraped_page.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Remove common unnecessary elements\n",
    "for tag in soup([\"script\", \"style\", \"footer\", \"nav\", \"aside\", \"meta\", \"link\", \"svg\"]):\n",
    "    tag.extract()\n",
    "\n",
    "# Remove hidden elements (e.g., display:none)\n",
    "for tag in soup.find_all(style=lambda value: value and \"display:none\" in value):\n",
    "    tag.extract()\n",
    "\n",
    "# Remove empty tags (helps clean up clutter)\n",
    "for tag in soup.find_all(lambda tag: not tag.text.strip() and not tag.name in [\"img\", \"br\"]):\n",
    "    tag.extract()\n",
    "\n",
    "# Convert cleaned HTML to string\n",
    "cleaned_html = soup.prettify()\n",
    "\n",
    "# Save cleaned HTML\n",
    "with open(\"html/cleaned_page.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_html)\n",
    "\n",
    "print(\"‚úÖ Cleaned HTML saved to cleaned_page.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full Markdown file saved: full_extracted.md\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import markdownify\n",
    "\n",
    "# Load full HTML\n",
    "with open(\"html/cleaned_page.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Convert to Markdown\n",
    "markdown_content = markdownify.markdownify(str(soup), heading_style=\"ATX\")  # Uses # for headings\n",
    "\n",
    "# Save as one file\n",
    "with open(\"markdown/full_extracted.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(markdown_content)\n",
    "\n",
    "print(\"‚úÖ Full Markdown file saved: full_extracted.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompted chatgpt to extract relevant information regarding furniture only this page is for product named \"daze\" i want information related to it only \n",
    "#### then it gave me furniture_data.json\n",
    "\n",
    "Use Crawl4AI and create llm strategy to convert html to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'structured_daze_data.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"jsons/furniture_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "# Initialize structured dictionary\n",
    "product_data = {\n",
    "    \"product_name\": None,\n",
    "    \"description\": None,\n",
    "    \"designer\": None,\n",
    "    \"year\": None,\n",
    "    \"dimensions\": None,\n",
    "    \"materials_and_finishes\": None,\n",
    "    \"images\": [],\n",
    "    \"designer_bio\": None\n",
    "}\n",
    "\n",
    "# Extract product name\n",
    "for key in raw_data:\n",
    "    if \"Daze\" in key:\n",
    "        product_data[\"product_name\"] = key.strip(\"# \")\n",
    "\n",
    "# Extract description\n",
    "for key, value in raw_data.items():\n",
    "    if \"Architectural shapes\" in key:  # Recognizing description structure\n",
    "        product_data[\"description\"] = key\n",
    "\n",
    "# Extract designer and year\n",
    "for key, value in raw_data.items():\n",
    "    if \"Designer\" in key:\n",
    "        product_data[\"designer\"] = key.replace(\"Designer: \", \"\").strip()\n",
    "    if \"Year\" in key:\n",
    "        product_data[\"year\"] = value.replace(\"‚ñ≥\", \"\").strip()\n",
    "\n",
    "# Extract dimensions\n",
    "for key, value in raw_data.items():\n",
    "    if \"Dimensions\" in key:\n",
    "        product_data[\"dimensions\"] = value.replace(\"‚ñ≥\", \"\").strip()\n",
    "\n",
    "# Extract materials and finishes\n",
    "for key, value in raw_data.items():\n",
    "    if \"Materials and finishes\" in key:\n",
    "        product_data[\"materials_and_finishes\"] = value.strip()\n",
    "\n",
    "# Extract images\n",
    "for key, value in raw_data.items():\n",
    "    if key.startswith(\"![](\"):  # Image Markdown format\n",
    "        image_url = key.replace(\"![](\", \"\").replace(\")\", \"\").strip()\n",
    "        product_data[\"images\"].append(image_url)\n",
    "\n",
    "# Extract designer bio (Search both keys and values)\n",
    "for key, value in raw_data.items():\n",
    "    if \"Truly Truly\" in key or \"Truly Truly\" in str(value):\n",
    "        product_data[\"designer_bio\"] = value.strip()\n",
    "\n",
    "# Convert to JSON format\n",
    "final_json_path = \"jsons/structured_daze_data.json\"\n",
    "with open(final_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(product_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "final_json_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
